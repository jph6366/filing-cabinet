{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5268e259-6756-45c8-8311-307c1e598b64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset MNIST:\n",
       "  metadata  =>    Dict{String, Any} with 3 entries\n",
       "  split     =>    :train\n",
       "  features  =>    28×28×60000 Array{Float32, 3}\n",
       "  targets   =>    60000-element Vector{Int64}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLDatasets\n",
    "\n",
    "MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d200de-15ed-4c05-8050-9844fa22a2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28, 28, 60000), (0.0f0, 1.0f0))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = MNIST()\n",
    "X,y =  digits.features, digits.targets\n",
    "size(X), extrema(X)\n",
    "# we can see that the unit range has been normalized\n",
    "# also known as min-max scaling which scales features\n",
    "# to lie in the interval [0; 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46c2d5d-4a8a-4828-825c-a2a57d3cc7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"28×28×60000 Array{Float32, 3}\", \"60000-element Vector{Int64}\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = MNIST(:train)\n",
    "# or call digits.split\n",
    "Xtrain, ytrain = trainset[:] # return all observations\n",
    "summary(Xtrain), summary(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4319b90b-fd5f-41e7-9b60-df2ec17068c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"28×28×10000 Array{Float32, 3}\", \"10000-element Vector{Int64}\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = MNIST(:test)\n",
    "Xtest, ytest = testset[:]\n",
    "summary(Xtest), summary(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db6dfd6c-a92c-4ac1-8ac5-342c7e3db320",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, OneHotArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c369667c-3c7b-4933-9660-837b6d6e5ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×60000 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n",
       " ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  …  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  1     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  …  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  1  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  1\n",
       " ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fluxonehoty = onehotbatch(ytrain, 0:9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c693c6b-c8d2-467d-9e94-5ed712f20d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model (generic function with 1 method)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "W = rand(Float32, 3, 4)\n",
    "b = [0.0f0, 0.0f0, 0.0f0]\n",
    "m(W, b, X) = W * X .+ b\n",
    "\n",
    "softmax(X) = exp.(X) ./ sum(exp.(X), dims=1)\n",
    "model(W, b, X) = softmax(m(W, b, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5217df00-5e16-4015-b61f-2de6830b102c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784 => 10),                     \u001b[90m# 7_850 parameters\u001b[39m\n",
       "  Main.softmax,\n",
       ") "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLUtils\n",
    "\n",
    "Xtrainflat = flatten(Xtrain)\n",
    "# 28 x 28 = 284\n",
    "fluxmodel = Chain(Dense(784 => 10), softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9962eac2-a5e6-4c7c-ae00-f474ff2b3ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.30521f0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLUtils\n",
    "\n",
    "function lossfunction(fluxmodel, features, onehotlabels)\n",
    "    ŷ = fluxmodel(features)\n",
    "    Flux.logitcrossentropy(ŷ, onehotlabels)\n",
    "end\n",
    "lossfunction(fluxmodel, Xtrainflat, fluxonehoty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b53fd15-a986-4b9f-9341-92cd0885062d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1182"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Statistics\n",
    "function trainmodel!(loss, model, features, labels)\n",
    "    dLdm, _, _ = gradient(loss, model, features, labels)\n",
    "    @. model[1].weight = model[1].weight - 0.000001 * dLdm[:layers][1][:weight]\n",
    "    @. model[1].bias = model[1].bias - 0.000001 * dLdm[:layers][1][:bias]\n",
    "end\n",
    "\n",
    "\n",
    "trainmodel!(lossfunction, fluxmodel, Xtrainflat, fluxonehoty)\n",
    "mean(Flux.onecold(fluxmodel(Xtrainflat), 0:9) .== ytrain )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e726fb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss initialized at 2.3488715\n",
      "2.34887\n",
      "validation score: 2.3505385\n",
      "loss initialized at 2.3256462\n",
      "2.3256445\n",
      "validation score: 2.3309221\n",
      "loss initialized at 2.4060252\n",
      "2.4060235\n",
      "validation score: 2.4090607\n",
      "loss initialized at 2.3913388\n",
      "2.3913374\n",
      "validation score: 2.3935323\n",
      "loss initialized at 2.3357458\n",
      "2.3357444\n",
      "validation score: 2.3286624\n"
     ]
    }
   ],
   "source": [
    "function trainmodel!(loss, model, features, labels)\n",
    "   dLdm, _, _ = gradient(loss, model, features, labels)\n",
    "   @. model.weight = model.weight - 0.000001 * dLdm.weight\n",
    "   @. model.bias = model.bias - 0.000001 * dLdm.bias\n",
    "end\n",
    "\n",
    "for ((xtrain, ytrain),(xval, yval)) in MLUtils.kfolds(shuffleobs((X,onehotbatch(y, 0:9))), k=5)\n",
    "    # MLUtils.flatten = reshape(x, :, size(x)[end])\n",
    "    xnorm = flatten(xtrain)\n",
    "\n",
    "    xnval = flatten(xval)\n",
    "    \n",
    "    model = Dense(784 => 10)\n",
    "\n",
    "    # run an infinite loop that breaks once change in loss is < δ\n",
    "    loss_init = Inf;\n",
    "    while true\n",
    "        trainmodel!(lossfunction, model, xnorm, ytrain)\n",
    "        # intialize loss value\n",
    "        if loss_init == Inf\n",
    "            loss_init = lossfunction(model, xnorm, ytrain)\n",
    "            println(\"loss initialized at \", loss_init)\n",
    "            continue\n",
    "        end\n",
    "        # convergence check: break if change in loss is <  (1 / 10³)\n",
    "        if abs(loss_init - lossfunction(model, xnorm, ytrain)) < 1e-4\n",
    "            break\n",
    "        else\n",
    "            loss_init = lossfunction(model, xnorm, ytrain)\n",
    "        end\n",
    "    end\n",
    "    println(lossfunction(model, xnorm, ytrain))\n",
    "    println(\"validation score: \", lossfunction(model, xnval, yval))\n",
    "end\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  },
  "name": "Untitled2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
